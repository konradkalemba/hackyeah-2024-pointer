# -*- coding: utf-8 -*-
"""TXT-analiza

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bB4n3RXqcVLLF_gwoXt3Ji9wjkuoyY2c
"""

!pip install spacy
!pip install sentence-transformers
!pip install langdetect
!pip install nltk
!python -m spacy download pl_core_news_sm
!pip install morfeusz2
!pip install googletrans==4.0.0-rc1

import re
from sentence_transformers import SentenceTransformer, util
from langdetect import detect
import spacy
import nltk
from transformers import BertTokenizer, BertModel, pipeline
import torch
import morfeusz2
from googletrans import Translator
import time
import json

translator = Translator()
morf = morfeusz2.Morfeusz()

# Upewnij się, że masz odpowiednie dane dla nltk
nltk.download('words')
from nltk.corpus import words

# Inicjalizacja modelu Zero-Shot Classification
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Lista kategorii tematycznych
categories = [
     "Economy", "Politics", "Technology", "Health",
"Education", "Science", "Culture", "Transportation", "Environment", "Law",
"Entertainment", "Finance", "Business", "Religion"]

# Załadowanie modelu SentenceTransformer do analizy zmian tematu
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
# Załadowanie modelu spaCy do analizy składniowej
nlp = spacy.load("pl_core_news_sm")

def classify_sentence_topic(sentence):
    # Added a delay and error handling to the translation
    try:
        time.sleep(2) # Wait for one second before making the request
        translated_sentence = translator.translate(sentence, dest='en').text
        print(translated_sentence)
    except Exception as e:
        print(f"Error translating sentence: {e}")
        return "Unknown" # Return a default value if translation fails
    # Klasyfikacja z użyciem zero-shot classification
    result = classifier(translated_sentence, candidate_labels=categories)
    # Zwracamy kategorię z najwyższym prawdopodobieństwem
    return result['labels'][0]

def detect_topic_change_classifier_advanced(text):
    # Tokenizacja na zdania za pomocą spaCy
    doc = nlp(text)
    sentences = [sent.text.strip() for sent in doc.sents]

    classified_topics = []
    for sent in sentences:
        if sent:
            topic = classify_sentence_topic(sent)
            # Dodanie sprawdzenia, czy topic jest None
            if topic is None:
                print(f"Error classifying sentence: {sent}")
                return [] # Przerwanie funkcji, jeśli klasyfikacja się nie powiodła
            classified_topics.append(topic)

    topic_changes = []
    similarity_threshold = 0

    if len(sentences) > 1:
        previous_embedding = None
        for i in range(len(sentences)):
            try:
                translated = translator.translate(sentences[i], dest='en').text
                current_embedding = model.encode(translated)
            except Exception as e:
                print(f"Error translating sentence: {sentences[i]} - {e}")
                current_embedding = None
            try:
              if previous_embedding is not None and current_embedding is not None:
                similarity = util.cos_sim(previous_embedding, current_embedding)[0][0]
                if classified_topics[i] != classified_topics[i-1] and similarity < similarity_threshold:
                  print(f"Zmiana tematu między zdaniem {i-1} a {i}: {classified_topics[i-1]} -> {classified_topics[i]} (Similarity: {similarity})")
                  topic_changes.append(i - 1) # Odejmowanie 1 od indeksu
                  topic_changes.append(i)
            except Exception as e:
              print(f"err")
            if current_embedding is not None: # Przypisanie tylko wtedy, gdy tłumaczenie się powiodło
                previous_embedding = current_embedding

    return topic_changes

# Funkcja do sprawdzenia, czy słowo jest polskim słownikiem
def is_polish_word(word):
  analysis = morf.analyse(word)
  return analysis[0][2][2] != 'ign'  # Check if the word is not ignored (unknown)


# Funkcja do detekcji powtórzeń
def detect_repetitions(transcription, n=1):
    transcription = re.sub(r'[^\w\s]', '', transcription.lower())
    words = transcription.split()
    repeated_ngrams = []
    for i in range(len(words) - n + 1):
        ngram = tuple(words[i:i+n])
        if i < len(words) - n and ngram == tuple(words[i+n:i+2*n]):
            if i not in repeated_ngrams:
                repeated_ngrams.append(i)
    return repeated_ngrams

# Funkcja do detekcji żargonu
def detect_jargon(text):
    jargon_terms = [
        "blockchain", "AI", "deep learning", "synergia", "algorytm",
        "token", "big data", "cloud computing", "data science", "sztuczna inteligencja",
        "uczenie maszynowe", "analiza danych", "automatyzacja", "fintech"
        # Możesz dodać więcej terminów lub zaimplementować AI do generowania żargonu
    ]

    words = re.findall(r'\b\w+\b', text.lower())
    jargon_used = list(set(word for word in words if word in jargon_terms))
    return jargon_used

# Funkcja do wykrywania nieistniejących słów
def detect_nonexistent_words(text):
    words_in_text = set(re.findall(r'\b\w+\b', text.lower()))
    nonexistent_words = []

    for word in words_in_text:
        # Ignore date-like patterns
        if re.match(r'^\d+[\-/]\d+[\-/]\d+$', word):
            continue

        # Sprawdzenie istnienia słowa w słowniku spaCy dla języka polskiego (z uwzględnieniem wielkości liter)
        if word.lower() not in nlp.vocab and not is_polish_word(word):
            nonexistent_words.append(word)

    return list(set(nonexistent_words))  # Usunięcie duplikatów

# Funkcja do wykrywania języka innego niż polski
def detect_non_polish_language(text):
    detected_language = detect(text)
    if detected_language != 'pl':
        return detected_language
    return None  # Jeśli jest w języku polskim

# Funkcja do wykrywania strony biernej
def detect_passive_voice(text):
    doc = nlp(text)
    passive_sentences = [sent.text for sent in doc.sents if any(token.dep_ == 'passive' for token in sent)]

    for sent in doc.sents:
        for token in sent:
            if token.text.endswith("no") or token.text.endswith("to") or token.text.endswith("ano") or token.text.endswith("iono"):
                passive_sentences.append(token.text)

    return passive_sentences

def is_word_too_long(word):
  vowels = "aąeęioóuy"
  syllable_count = 0
  for char in word.lower():
    if char in vowels:
      syllable_count += 1
  return syllable_count >= 4

def is_number_too_long(number):
  number_str = str(number)
  if len(number_str) <= 6:
    return False
  zero_count = number_str.count('0')
  return zero_count < len(number_str) / 2

# Główna funkcja do analizy transkrypcji
def analyze_transcription(transcription):
    results = {}
    words = transcription.split()

    # Detekcja powtórzeń
    results['repetitions'] = detect_repetitions(transcription)

  # Detekcja zmian tematu - indeks pierwszego słowa
    topic_changes = detect_topic_change_classifier_advanced(transcription)
    doc = nlp(transcription)
    sentences = [sent.text.strip() for sent in doc.sents]

    results['topic_changes'] = []
    for i in topic_changes:
        if i < len(sentences) - 1:  # Sprawdzenie, czy indeks jest w zakresie
            sentence = sentences[i]
            first_word = sentence.split()[0]
            try:
                index = words.index(first_word)
                results['topic_changes'].append(index)
            except ValueError:
                print(f"Słowo '{first_word}' nie znalezione w transkrypcji.")

    # Usunięcie wyników na parzystych indeksach
    results['topic_changes'] = [results['topic_changes'][i] for i in range(len(results['topic_changes'])) if i % 2 != 0]


    # Detekcja żargonu
    results['jargon'] = detect_jargon(transcription)
    # Detekcja strony biernej
    results['passive_voice'] = detect_passive_voice(transcription)

    # Detekcja nieistniejących słów
    nonexistent_words = detect_nonexistent_words(transcription)

    # Detekcja innego języka
    non_polish_words = []
    for word in nonexistent_words:
        try:
            if detect(word) != 'pl':
                non_polish_words.append(word)
        except:
            pass

    # Usunięcie słów z nonexistent_words, które są w non_polish_language (ignorując wielkość liter)
    nonexistent_words = [word for word in nonexistent_words if word.lower() not in [w.lower() for w in non_polish_words]]

    # Usunięcie słów z nonexistent_words, które są w passive_voice (ignorując wielkość liter)
    nonexistent_words = [word for word in nonexistent_words if word.lower() not in [w.lower() for w in results['passive_voice']]]

    results['nonexistent_words'] = nonexistent_words
    results['non_polish_language'] = non_polish_words

    # Detekcja zbyt długich słów
    results['long_words'] = [word for word in re.findall(r'\b\w+\b', transcription) if is_word_too_long(word)]

    # Detekcja zbyt długich liczb
    results['long_numbers'] = [number for number in re.findall(r'\d+', transcription) if is_number_too_long(int(number))]

    return results

# Przykładowa transkrypcja
transcription = """
Wczoraj byłem na spacerze w parku. Słońce świeciło, ptaki śpiewały, a ludzie relaksowali się na ławkach. Nagle zauważyłem małego psa, który biegał bez smyczy. Podszedłem bliżej i zobaczyłem, że ma obrożę z imieniem "Burek". Postanowiłem poszukać jego właściciela.
Inflacja w Polsce rośnie, a ceny produktów spożywczych są coraz wyższe. Rząd zapowiada wprowadzenie nowych programów socjalnych, aby złagodzić skutki drożyzny dla najuboższych. Eksperci ostrzegają jednak, że może to doprowadzić do dalszego wzrostu inflacji.
"""
# Analiza transkrypcji
report = analyze_transcription(transcription)
json_report = json.dumps(report)

print(json_report)

